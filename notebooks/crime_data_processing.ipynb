{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Data Exploration\n",
    "\n",
    "We're going to explore the crime data. Specifically, we're exploring a specific aspect of the data that's been problematic: the distribution of crimes over time. Recently, when attempting to predict the crime rate on the date March 3, 2016, I found that--apparently--there were no crimes within the last 31 days! Obviously, I'm not aware of the dates for which we have crime data. Time for some exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Load the data as an RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(CMPLNT_NUM=101109527, CMPLNT_FR_DT=u'12/31/2015', CMPLNT_FR_TM=u'23:45:00', CMPLNT_TO_DT=None, CMPLNT_TO_TM=None, RPT_DT=u'12/31/2015', KY_CD=113, OFNS_DESC=u'FORGERY', PD_CD=729, PD_DESC=u'FORGERY,ETC.,UNCLASSIFIED-FELO', CRM_ATPT_CPTD_CD=u'COMPLETED', LAW_CAT_CD=u'FELONY', JURIS_DESC=u'N.Y. POLICE DEPT', BORO_NM=u'BRONX', ADDR_PCT_CD=44, LOC_OF_OCCUR_DESC=u'INSIDE', PREM_TYP_DESC=u'BAR/NIGHT CLUB', PARKS_NM=None, HADEVELOPT=None, X_COORD_CD=1007314, Y_COORD_CD=241257, Latitude=40.828848333, Longitude=-73.916661142, Lat_Lon=u'(40.828848333, -73.916661142)')\n",
      "[101109527, u'12/31/2015', u'23:45:00', None, None, u'12/31/2015', 113, u'FORGERY', 729, u'FORGERY,ETC.,UNCLASSIFIED-FELO', u'COMPLETED', u'FELONY', u'N.Y. POLICE DEPT', u'BRONX', 44, u'INSIDE', u'BAR/NIGHT CLUB', None, None, 1007314, 241257, 40.828848333, -73.916661142, u'(40.828848333, -73.916661142)']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark.sql as sql\n",
    "\n",
    "# From https://stackoverflow.com/a/36218558 .\n",
    "def sparkImport(module_name, module_directory):\n",
    "    \"\"\"\n",
    "    Convenience function. \n",
    "    \n",
    "    Tells the SparkContext sc (must already exist) to load\n",
    "    module module_name on every computational node before\n",
    "    executing an RDD. \n",
    "    \n",
    "    Args:\n",
    "        module_name: the name of the module, without \".py\". \n",
    "        module_directory: the path, absolute or relative, to\n",
    "                          the directory containing module\n",
    "                          module_Name. \n",
    "    \"\"\"\n",
    "    module_path = os.path.abspath(\n",
    "        module_directory + \"/\" + module_name + \".py\")\n",
    "    sc.addPyFile(module_path)\n",
    "\n",
    "# Add all scripts from repository to local path. \n",
    "# From https://stackoverflow.com/a/35273613 .\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import preprocessing as pp\n",
    "\n",
    "sparkImport(\"preprocessing\", \"..\")\n",
    "\n",
    "ss = sql.SparkSession(sc)\n",
    "complaints_df = ss.read.csv(\"crime_complaints_with_header.csv\", inferSchema=True, header=True)\n",
    "complaints_rdd = complaints_df.rdd.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Counting\n",
    "\n",
    "We want to know how many crimes in total are in our dataset. We also want to know how many *valid* crimes (the ones we actually predict) are in our dataset. A crime is *valid* iff it starts and ends on the same calendar date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of crimes: 838153\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of crimes: \" + str(complaints_rdd.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid crimes: 710577\n"
     ]
    }
   ],
   "source": [
    "valid_complaints_rdd = complaints_rdd.filter(pp.complaint_is_valid)\n",
    "print(\"number of valid crimes: \" + str(valid_complaints_rdd.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We still have upwards of $80\\%$ of our crimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Over Time And Space\n",
    "\n",
    "For the model, it doesn't particularly matter what the distribution is over space and time. However, for evaluating the accuracy of our model's crime predictions, we want to be aware of when there is *absolutely no crime* during a particular day. If there is absolutely no crime on a given day, then--regardless of how the model predicts crime for each grid square on that day--choosing to catch crime in the top-rated squares will technically allow you to catch all crime for that day. In other words, *any* model will achieve perfect performance on a day in which no crime occurs.\n",
    "\n",
    "(Note that, when I say \"model\" here, I am talking about models which attempt to rank grid squares on a given day according to how much crime will occur in each square. This class of models includes our linear model.)\n",
    "\n",
    "So, let's plot this: the number of crimes per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 30.0 failed 1 times, most recent failure: Lost task 26.0 in stage 30.0 (TID 296, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
       "    process()\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
       "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 830, in func\n",
       "    initial = next(iterator)\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1239, in countPartition\n",
       "    for obj in iterator:\n",
       "  File \"<string>\", line 5, in <lambda>\n",
       "  File \"/tmp/spark-46992bef-5704-46fb-8700-dcbd47c3a0cc/userFiles-60c8b6cd-eb3c-4e7e-b180-0aa7e60b55b8/preprocessing.py\", line 21, in get_complaint_occurrence_day\n",
       "    (month, day, year) = [int(n) for n in complaint_record[complaint_field_index['from_date']].split('/')]\n",
       "AttributeError: 'NoneType' object has no attribute 'split'\n",
       "\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "\tat scala.Option.foreach(Option.scala:257)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n",
       "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
       "    process()\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
       "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 830, in func\n",
       "    initial = next(iterator)\n",
       "  File \"/share/apps/spark/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1239, in countPartition\n",
       "    for obj in iterator:\n",
       "  File \"<string>\", line 5, in <lambda>\n",
       "  File \"/tmp/spark-46992bef-5704-46fb-8700-dcbd47c3a0cc/userFiles-60c8b6cd-eb3c-4e7e-b180-0aa7e60b55b8/preprocessing.py\", line 21, in get_complaint_occurrence_day\n",
       "    (month, day, year) = [int(n) for n in complaint_record[complaint_field_index['from_date']].split('/')]\n",
       "AttributeError: 'NoneType' object has no attribute 'split'\n",
       "\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\t... 1 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n', JavaObject id=o201), <traceback object at 0x7fc4707dac20>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "complaints_per_day = complaints_rdd \\\n",
    "    .map(lambda record: (pp.get_complaint_occurrence_day(record), 1)) \\\n",
    "    .countByKey()\n",
    "sorted_counts = sorted(complaints_per_day.items())\n",
    "days, counts = zip(*sorted_counts)\n",
    "pyplot.plot(days, counts)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.1.0 / PySpark",
   "language": "python",
   "name": "spark2.1.0_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.11\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
