{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The New Pipeline\n",
    "\n",
    "This is a rough draft of our new code. We're using PySpark's DataFrame and Pipeline API (for the most part) to re-implement what we've already done, and then move forward. It's been much more efficient (from a human-time-spent perspective; not necessarily from time/space complexity perspective) to use thus far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql as sql\n",
    "\n",
    "ss = sql.SparkSession.builder.appName(\"TwitterTokenizing\")\\\n",
    "                             .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns:\n",
      "['timestamp', 'lon', 'lat', 'tweet']\n",
      "Sample row:\n",
      "[Row(timestamp=1435723208, lon=-73.951206, lat=40.79435, tweet=u'Incident on #VariousLocalExpressBuses SB from 5th Avenue:106th Street to 5th Avenue: 57th Street http://t.co/KrLOmkAqcE')]\n",
      "Number of tweets:\n",
      "38012483\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as types\n",
    "\n",
    "tweets_schema = types.StructType([\n",
    "  types.StructField('id', types.LongType()),\n",
    "  types.StructField('timestamp', types.LongType()),\n",
    "  types.StructField('postalCode', types.StringType()),\n",
    "  types.StructField('lon', types.DoubleType()),\n",
    "  types.StructField('lat', types.DoubleType()),\n",
    "  types.StructField('tweet', types.StringType()),\n",
    "  types.StructField('user_id', types.LongType()),\n",
    "  types.StructField('application', types.StringType()),\n",
    "  types.StructField('source', types.StringType())\n",
    "])\n",
    "tweets_df = ss.read.csv('tweets2.csv',\n",
    "                         escape='\"',\n",
    "                         header='true',\n",
    "                         schema=tweets_schema,\n",
    "                         mode='DROPMALFORMED')\n",
    "tweets_df = tweets_df.drop('id') \\\n",
    "                     .drop('postalCode') \\\n",
    "                     .drop('user_id') \\\n",
    "                     .drop('application') \\\n",
    "                     .drop('source')\n",
    "\n",
    "print('Dataframe columns:')\n",
    "print(tweets_df.columns)\n",
    "print('Sample row:')\n",
    "print(tweets_df.take(1))\n",
    "print('Number of tweets:')\n",
    "print(tweets_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      ":( :( :( Incident on #VariousLocalExpressBuses SB from 5th Avenue:106th Street to 5th Avenue: 57th Street http://t.co/KrLOmkAqcE\n",
      "Tokenized tweet:\n",
      "[u':(', u':(', u':(', u'Incident', u'on', u'#VariousLocalExpressBuses', u'SB', u'from', u'5th', u'Avenue', u':', u'106th', u'Street', u'to', u'5th', u'Avenue', u':', u'57th', u'Street', u'http://t.co/KrLOmkAqcE']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# From https://stackoverflow.com/a/36218558 .\n",
    "def sparkImport(module_name, module_directory):\n",
    "    \"\"\"\n",
    "    Convenience function. \n",
    "    \n",
    "    Tells the SparkContext sc (must already exist) to load\n",
    "    module module_name on every computational node before\n",
    "    executing an RDD. \n",
    "    \n",
    "    Args:\n",
    "        module_name: the name of the module, without \".py\". \n",
    "        module_directory: the path, absolute or relative, to\n",
    "                          the directory containing module\n",
    "                          module_Name. \n",
    "    \n",
    "    Returns: none. \n",
    "    \"\"\"\n",
    "    module_path = os.path.abspath(\n",
    "        module_directory + \"/\" + module_name + \".py\")\n",
    "    sc.addPyFile(module_path)\n",
    "\n",
    "# Add all scripts from repository to local path. \n",
    "# From https://stackoverflow.com/a/35273613 .\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import twokenize\n",
    "sparkImport(\"twokenize\", \"..\")\n",
    "\n",
    "print (\"Original tweet:\")\n",
    "example_tweet = u':( :( :( Incident on #VariousLocalExpressBuses SB from 5th Avenue:106th Street to 5th Avenue: 57th Street http://t.co/KrLOmkAqcE'\n",
    "print(example_tweet)\n",
    "print(\"Tokenized tweet:\")\n",
    "print(twokenize.tokenize(example_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'lon', 'lat', 'tweet', 'tokens']\n",
      "[Row(timestamp=1435723208, lon=-73.951206, lat=40.79435, tweet=u'Incident on #VariousLocalExpressBuses SB from 5th Avenue:106th Street to 5th Avenue: 57th Street http://t.co/KrLOmkAqcE', tokens=[u'Incident', u'on', u'#VariousLocalExpressBuses', u'SB', u'from', u'5th', u'Avenue', u':', u'106th', u'Street', u'to', u'5th', u'Avenue', u':', u'57th', u'Street', u'http://t.co/KrLOmkAqcE'])]\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "\n",
    "sql_tokenize = functions.udf(\n",
    "    lambda tweet: twokenize.tokenize(tweet),\n",
    "    returnType=types.ArrayType(types.StringType()))\n",
    "tweets_df = tweets_df \\\n",
    "    .withColumn(\"tweet_tokens\", sql_tokenize(tweets_df.tweet)) \\\n",
    "    .drop('tweet')\n",
    "\n",
    "print(tweets_df.columns)\n",
    "print(tweets_df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lon', 'lat', 'tweet', 'tokens', 'date']\n",
      "[Row(lon=-73.951206, lat=40.79435, tweet=u'Incident on #VariousLocalExpressBuses SB from 5th Avenue:106th Street to 5th Avenue: 57th Street http://t.co/KrLOmkAqcE', tokens=[u'Incident', u'on', u'#VariousLocalExpressBuses', u'SB', u'from', u'5th', u'Avenue', u':', u'106th', u'Street', u'to', u'5th', u'Avenue', u':', u'57th', u'Street', u'http://t.co/KrLOmkAqcE'], date=datetime.date(2015, 7, 1))]\n"
     ]
    }
   ],
   "source": [
    "date_column = tweets_df['timestamp'].cast(types.TimestampType()) \\\n",
    "                                    .cast(types.DateType())\n",
    "\n",
    "tweets_df = tweets_df.withColumn('date', date_column) \\\n",
    "                     .drop('timestamp')\n",
    "print(tweets_df.columns)\n",
    "print(tweets_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151306\n",
      "[Row(lon=-74.27470828, lat=40.59844873, tweet=u'This is me, YL a voice from JERSEY.. And im pushin this #blackpowermovement ! Links in my bio! #BlackHistoryMonth ! https://t.co/MLvAS9jIqj', tokens=[u'This', u'is', u'me', u',', u'YL', u'a', u'voice', u'from', u'JERSEY', u'..', u'And', u'im', u'pushin', u'this', u'#blackpowermovement', u'!', u'Links', u'in', u'my', u'bio', u'!', u'#BlackHistoryMonth', u'!', u'https://t.co/MLvAS9jIqj'], date=datetime.date(2016, 2, 1))]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "date_to_column = functions.lit(datetime.datetime(2016, 3, 3))\n",
    "date_from_column = functions.lit(functions.date_sub(date_to_column, 31))\n",
    "filtered_tweets_df = tweets_df.filter(\n",
    "    ~(tweets_df.date < date_from_column)\n",
    "    & (tweets_df.date < date_to_column))\n",
    "\n",
    "print(filtered_tweets_df.count())\n",
    "print(filtered_tweets_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.1.0 / PySpark",
   "language": "python",
   "name": "spark2.1.0_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.11\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
